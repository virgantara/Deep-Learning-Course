{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674e0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7757f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2eb574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000 Test size: 1000\n",
      "Train label: {1: 2500, 0: 2500}\n",
      "Test label: {0: 500, 1: 500}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/imdb.csv\")\n",
    "\n",
    "df[\"label\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "pos_data = df[df['label'] == 1].iloc[:2500]\n",
    "neg_data = df[df['label'] == 0].iloc[:2500]\n",
    "\n",
    "train_data = pd.concat([pos_data, neg_data]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "pos_test = df[df['label'] == 1].iloc[2500:3000]\n",
    "neg_test = df[df['label'] == 0].iloc[2500:3000]\n",
    "test_data = pd.concat([pos_test, neg_test]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_data), \"Test size:\", len(test_data))\n",
    "print(\"Train label:\", train_data['label'].value_counts().to_dict())\n",
    "print(\"Test label:\", test_data['label'].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "063517d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f3949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_data[\"review\"].apply(clean).tolist()\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "\n",
    "test_texts = test_data[\"review\"].apply(clean).tolist()\n",
    "test_labels = test_data[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021afbb",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf49558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589aae9",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0279785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Accuracy: 0.8946\n",
      "[Test]  Accuracy: 0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virgantara/anaconda3/envs/py39-env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, train_labels)\n",
    "\n",
    "train_preds = clf.predict(X_train)\n",
    "test_preds = clf.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(train_labels, train_preds)\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"[Train] Accuracy: {train_acc:.4f}\")\n",
    "print(f\"[Test]  Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467031a",
   "metadata": {},
   "source": [
    "# Saatnya Word Embeddings beraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8024d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for review in train_data[\"review\"]:\n",
    "    counter.update(tokenize(review))\n",
    "\n",
    "vocab_size = 10000\n",
    "special_tokens = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(vocab_size - 2))}\n",
    "vocab.update(special_tokens)\n",
    "\n",
    "def encode(text):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "maxlen = 20\n",
    "def collate_fn(batch):\n",
    "    inputs = [encode(sample[\"text\"])[:maxlen] for sample in batch]\n",
    "    inputs = [torch.cat([seq, torch.zeros(maxlen - len(seq), dtype=torch.long)]) if len(seq) < maxlen else seq for seq in inputs]\n",
    "    labels = [sample[\"label\"] for sample in batch]\n",
    "    return torch.stack(inputs), torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c965d",
   "metadata": {},
   "source": [
    "# Membangun IMBD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80870a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return {\"text\": row[\"review\"], \"label\": row[\"label\"]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "001dbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d0d87",
   "metadata": {},
   "source": [
    "# Load Pre-Trained GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a6dd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in tqdm(f,desc='Loading GloVe Embedding'):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float32)\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480997c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, embeddings_index, embedding_dim):\n",
    "    embedding_matrix = torch.randn(len(vocab), embedding_dim) * 0.01\n",
    "    embedding_matrix[vocab[\"<pad>\"]] = torch.zeros(embedding_dim)\n",
    "\n",
    "    for word, idx in tqdm(vocab.items(), desc=\"Sedang membangun Embedding Matrix\"):\n",
    "        if word in embeddings_index:\n",
    "            embedding_matrix[idx] = embeddings_index[word]\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447bf461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeIMDBClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim):\n",
    "        super().__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.rnn(x)\n",
    "        return self.sigmoid(self.fc(h_n.squeeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe1486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=16, hidden_dim=64, sequence_length=200):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)            # [batch, seq_len, embedding_dim]\n",
    "        output, (h_n, c_n) = self.lstm(embedded)  # h_n: [1, batch, hidden_dim]\n",
    "        h = self.dropout(h_n.squeeze(0))\n",
    "        out = self.fc(h)\n",
    "#         out = self.fc(h_n.squeeze(0))           # [batch, 1]\n",
    "        return self.sigmoid(out)                # [batch, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4da22f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embedding: 400000it [00:17, 23527.02it/s]\n",
      "Sedang membangun Embedding Matrix: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 146136.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "glove_path = \"pretrains/glove.6B/glove.6B.100d.txt\"\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "\n",
    "glove_index = load_glove_embeddings(glove_path)\n",
    "embedding_matrix = build_embedding_matrix(vocab, glove_index, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b4591",
   "metadata": {},
   "source": [
    "# Prepraing Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b266f6f",
   "metadata": {},
   "source": [
    "# Let's Train !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1 - Loss: 0.6937 | Acc: 0.5034\n",
      "[Test] Epoch 1 - Loss: 0.6928 | Acc: 0.5170\n",
      "[Train] Epoch 2 - Loss: 0.6907 | Acc: 0.5314\n",
      "[Test] Epoch 2 - Loss: 0.6921 | Acc: 0.5300\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = GloVeIMDBClassifier(embedding_matrix, hidden_dim)\n",
    "model = LSTMClassifier(vocab_size=10000, embedding_dim=16, hidden_dim=64, sequence_length=200)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        pred = model(x_batch).squeeze()\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend((pred > 0.5).int().tolist())\n",
    "        all_labels.extend(y_batch.int().tolist())\n",
    "\n",
    "    train_acc = accuracy_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    print(f\"[Train] Epoch {epoch+1} - Loss: {avg_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    all_preds_test = []\n",
    "    all_labels_test = []\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            pred = model(x_batch).squeeze()\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            \n",
    "            all_preds_test.extend((pred > 0.5).int().tolist())\n",
    "            all_labels_test.extend(y_batch.int().tolist())\n",
    "\n",
    "    test_acc = accuracy_score(all_labels_test, all_preds_test)\n",
    "    \n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    print(f\"[Test] Epoch {epoch+1} - Loss: {avg_test_loss:.4f} | Acc: {test_acc:.4f}\")\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plotting Accuracy ---\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777340cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
